{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efff75db",
   "metadata": {},
   "source": [
    "# Kevin Ahmadiputra - TP058396"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184416c4",
   "metadata": {},
   "source": [
    "# Q1 - Form Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8134205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics. This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered. So what should a brand do to capture that low hanging fruit?\n"
     ]
    }
   ],
   "source": [
    "#Read Data from Data_1.txt file\n",
    "t1 = open(\"Data_1.txt\", \"r\")\n",
    "data = t1.read()\n",
    "t1.close()\n",
    " \n",
    "#Display data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c116b",
   "metadata": {},
   "source": [
    "## 1.1 Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e384b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations.', 'However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics.', 'This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered.', 'So what should a brand do to capture that low hanging fruit?']\n"
     ]
    }
   ],
   "source": [
    "#Code to install NLTK library     \n",
    "#!pip install nltk\n",
    "\n",
    "#Import library\n",
    "import nltk\n",
    "\n",
    "#Read data\n",
    "tk = nltk.sent_tokenize(data)\n",
    "\n",
    "#Display the result\n",
    "print(tk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95918766",
   "metadata": {},
   "source": [
    "## 1.2 Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2aa7f2",
   "metadata": {},
   "source": [
    "### 1.2.1 Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e85707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', '\"contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information\"', 'in', 'source', 'material,', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand,', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations.', 'However,', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit?']\n"
     ]
    }
   ],
   "source": [
    "#Split the sentences and assign to variable\n",
    "split_tk = [data][0].split()\n",
    "\n",
    "#Display split result\n",
    "print(split_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11079ae",
   "metadata": {},
   "source": [
    "### 1.2.2 Regular Expression (re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45bafe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', 'in', 'source', 'material', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', 'However', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "#Import Library\n",
    "import re\n",
    "\n",
    "#Use re.findall or .findall() module\n",
    "reg_tk = re.findall(\"[\\w]+\", data)\n",
    "\n",
    "#Display the result\n",
    "print(reg_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda587ec",
   "metadata": {},
   "source": [
    "### 1.2.3 NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfb76ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', '``', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', \"''\", 'in', 'source', 'material', ',', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', '.', 'However', ',', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', '.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', '.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "#Import Library\n",
    "import nltk\n",
    "\n",
    "#Use word_tokenize module then assign to variable\n",
    "nltk_tk = nltk.word_tokenize(data)\n",
    "\n",
    "#Display the result\n",
    "print(nltk_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8eb06",
   "metadata": {},
   "source": [
    "# Q2 - Form Word Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2d107",
   "metadata": {},
   "source": [
    "## 2.1 Regular Expression Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "975099c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysi', 'is', '``', 'contextual', 'min', 'of', 'text', 'which', 'identifie', 'and', 'extract', 'subjectiv', 'information', \"''\", 'in', 'sourc', 'material', ',', 'and', 'help', 'a', 'busines', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'servic', 'whil', 'monitor', 'onlin', 'conversation', '.', 'however', ',', 'analysi', 'of', 'social', 'media', 'stream', 'is', 'usually', 'restrict', 'to', 'just', 'basic', 'sentiment', 'analysi', 'and', 'count', 'bas', 'metric', '.', 'thi', 'is', 'akin', 'to', 'just', 'scratch', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'thos', 'high', 'valu', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discover', '.', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'captur', 'that', 'low', 'hang', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "#Import reg expression stemmer package \n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Assign the parameters to reg exp stemmer func\n",
    "reg_exp = RegexpStemmer('ing$|s$|e$|able$|ed$', min=4)\n",
    "\n",
    "#Word tokenize required to stem each words before stemming progress\n",
    "words = word_tokenize(data)\n",
    "\n",
    "#Stemming the words using RegEx Stemmer\n",
    "regstem = [reg_exp.stem(w.lower()) for w in words]\n",
    "\n",
    "#Display the result \n",
    "print(regstem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63818c",
   "metadata": {},
   "source": [
    "## 2.2 NLTK Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974f318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysi', 'is', '``', 'contextu', 'mine', 'of', 'text', 'which', 'identifi', 'and', 'extract', 'subject', 'inform', \"''\", 'in', 'sourc', 'materi', ',', 'and', 'help', 'a', 'busi', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'servic', 'while', 'monitor', 'onlin', 'convers', '.', 'howev', ',', 'analysi', 'of', 'social', 'media', 'stream', 'is', 'usual', 'restrict', 'to', 'just', 'basic', 'sentiment', 'analysi', 'and', 'count', 'base', 'metric', '.', 'thi', 'is', 'akin', 'to', 'just', 'scratch', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'those', 'high', 'valu', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discov', '.', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'captur', 'that', 'low', 'hang', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "#Import reg expression stemmer package \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Assign the parameters to porter stemmer func\n",
    "por_stem = PorterStemmer()\n",
    "\n",
    "#Word tokenize required to stem each words before stemming progress\n",
    "words = word_tokenize(data)\n",
    "\n",
    "#Stemming the words\n",
    "porter =[por_stem.stem(w) for w in words]\n",
    "\n",
    "#Display Result\n",
    "print(porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59e26e",
   "metadata": {},
   "source": [
    "# Q3 - Filter Stop Words and Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177e7e7",
   "metadata": {},
   "source": [
    "## 3.1 Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2912d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to download the stopwards package from NLKT library\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05d8fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'if', 'such', 'in', 'themselves', 'do', 't', 'can', \"wouldn't\", 'all', 'does', 'here', 'further', 'before', 'their', 'with', 'myself', 'have', 's', 'she', 'other', 'each', 'too', 'ain', 'needn', \"you're\", \"weren't\", 'theirs', 'having', 'between', \"shan't\", 'we', 'was', 'won', 'which', 've', 'until', 'than', \"won't\", 'don', 'd', 'our', 'isn', 'during', \"it's\", 'when', 'while', 'aren', 'those', 'from', 'should', 'mustn', 'is', 'nor', 'will', \"hadn't\", 'out', 'yourselves', 'few', 'there', 'what', 'against', 'why', 'his', 'mightn', 'hadn', 'just', 'hasn', 'weren', 'll', \"didn't\", 'were', 'me', 'down', \"shouldn't\", 'into', \"aren't\", 'most', 'wouldn', 'more', 'only', \"mustn't\", \"haven't\", 'both', \"hasn't\", 're', 'i', 'my', 'didn', 'off', 'by', 'did', 'that', 'ours', 'because', 'are', 'these', 'hers', \"doesn't\", \"you'd\", 'he', 'wasn', 'it', 'this', \"that'll\", 'haven', 'on', \"you'll\", 'for', 'being', 'own', 'as', 'be', 'doing', 'any', 'but', 'at', 'above', 'where', 'they', 'm', 'himself', \"wasn't\", 'very', 'an', 'and', 'after', 'yours', \"couldn't\", \"mightn't\", 'ourselves', 'about', 'once', 'a', 'has', \"don't\", 'y', 'doesn', 'shan', 'its', 'your', 'no', 'them', 'of', 'shouldn', \"she's\", 'then', 'not', 'over', 'couldn', 'ma', 'him', 'am', \"you've\", 'the', 'whom', 'up', 'had', 'or', 'o', 'again', \"should've\", 'some', 'under', 'same', 'how', 'been', \"needn't\", 'yourself', \"isn't\", 'her', 'so', 'to', 'herself', 'now', 'through', 'itself', 'you', 'below', 'who'}\n"
     ]
    }
   ],
   "source": [
    "#Import Stopwords Package from NLTK corpus\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#List down all english stopwords in object\n",
    "stop_w= set(stopwords.words('english'))\n",
    "\n",
    "#Display the result\n",
    "print(stop_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1934bfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysis', '``', 'contextual', 'mining', 'text', 'identifies', 'extracts', 'subjective', 'information', \"''\", 'source', 'material', ',', 'helping', 'business', 'understand', 'social', 'sentiment', 'brand', ',', 'product', 'service', 'monitoring', 'online', 'conversations', '.', 'however', ',', 'analysis', 'social', 'media', 'streams', 'usually', 'restricted', 'basic', 'sentiment', 'analysis', 'count', 'based', 'metrics', '.', 'akin', 'scratching', 'surface', 'missing', 'high', 'value', 'insights', 'waiting', 'discovered', '.', 'brand', 'capture', 'low', 'hanging', 'fruit', '?']\n",
      "\n",
      "sentiment analysis `` contextual mining text identifies extracts subjective information '' source material , helping business understand social sentiment brand , product service monitoring online conversations . however , analysis social media streams usually restricted basic sentiment analysis count based metrics . akin scratching surface missing high value insights waiting discovered . brand capture low hanging fruit ?\n"
     ]
    }
   ],
   "source": [
    "#Convert the text to list of words\n",
    "list_str = nltk.word_tokenize(data.lower())\n",
    "#print (list_str)\n",
    "\n",
    "#Create a list to store all detected stop words in txt corpus\n",
    "stop_w_found = []\n",
    "\n",
    "#Remove the stopwords\n",
    "no_stop_w_str = []\n",
    "for i in list_str:\n",
    "    #Append the detected stop words in text corpus to the created list\n",
    "    if i in stop_w:\n",
    "        stop_w_found.append(i)\n",
    "        \n",
    "    #Add non-stop words into the declared list variable\n",
    "    if not i in stop_w:\n",
    "        no_stop_w_str.append(i)\n",
    "\n",
    "#Display the txt corpus without any stop words in list & string format\n",
    "print(no_stop_w_str)\n",
    "print (\"\\n\"+\" \".join(no_stop_w_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c51ad061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'of', 'which', 'and', 'in', 'and', 'a', 'to', 'the', 'of', 'their', 'or', 'while', 'of', 'is', 'to', 'just', 'and', 'this', 'is', 'to', 'just', 'the', 'and', 'out', 'on', 'those', 'that', 'are', 'to', 'be', 'so', 'what', 'should', 'a', 'do', 'to', 'that']\n"
     ]
    }
   ],
   "source": [
    "#Display the list of discovered stop words in txt corpus\n",
    "print(stop_w_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be67fc",
   "metadata": {},
   "source": [
    "## 3.2 Punctuations removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5756f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysis', 'contextual', 'mining', 'text', 'identifies', 'extracts', 'subjective', 'information', 'source', 'material', 'helping', 'business', 'understand', 'social', 'sentiment', 'brand', 'product', 'service', 'monitoring', 'online', 'conversations', 'however', 'analysis', 'social', 'media', 'streams', 'usually', 'restricted', 'basic', 'sentiment', 'analysis', 'count', 'based', 'metrics', 'akin', 'scratching', 'surface', 'missing', 'high', 'value', 'insights', 'waiting', 'discovered', 'brand', 'capture', 'low', 'hanging', 'fruit']\n",
      "\n",
      "sentiment analysis contextual mining text identifies extracts subjective information source material helping business understand social sentiment brand product service monitoring online conversations however analysis social media streams usually restricted basic sentiment analysis count based metrics akin scratching surface missing high value insights waiting discovered brand capture low hanging fruit\n"
     ]
    }
   ],
   "source": [
    "#Remove punctuations\n",
    "punc =[',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%','`', '``', \"''\", '--', '...']\n",
    "no_punc_str = []\n",
    "\n",
    "for i in no_stop_w_str:\n",
    "    if i not in punc:\n",
    "        no_punc_str.append(i)\n",
    "        \n",
    "#Display Result in list and string format\n",
    "print(no_punc_str)\n",
    "print (\"\\n\"+\" \".join(no_punc_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b34e82",
   "metadata": {},
   "source": [
    "# Q4 - Form Parts of Speech (POS) Taggers & Syntatic Analysers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "207036b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A videogame or computergame is an electronic-game that involves interaction with a user interface or input device\n"
     ]
    }
   ],
   "source": [
    "#Read Data from Data_2.txt file\n",
    "t2 = open(\"Data_2.txt\", \"r\")\n",
    "data2 = t2.read()\n",
    "t2.close()\n",
    "\n",
    "#Display the txt inside data2 variable\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397926f",
   "metadata": {},
   "source": [
    "## 4.1 NLTK POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f47b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('videogame', 'NN'), ('or', 'CC'), ('computergame', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('electronic-game', 'JJ'), ('that', 'WDT'), ('involves', 'VBZ'), ('interaction', 'NN'), ('with', 'IN'), ('a', 'DT'), ('user', 'JJ'), ('interface', 'NN'), ('or', 'CC'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Import NLTK Library then import pos tag from NLTK library\n",
    "import nltk\n",
    "#Code to download required packages for NLTK POS Tagger\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "\n",
    "#Perform word tokenize in order to enable each word tagging\n",
    "tk = word_tokenize(data2) \n",
    "\n",
    "#Apply POS tags to tag each words\n",
    "pos_tk = pos_tag(tk) \n",
    "\n",
    "#Display the result\n",
    "print(pos_tk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2108da7",
   "metadata": {},
   "source": [
    "## 4.2 Regular Expression Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb3f3eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'AT'), ('videogame', 'NN'), ('or', 'NN'), ('computergame', 'NN'), ('is', 'NNS'), ('an', 'AT'), ('electronic-game', 'NN'), ('that', 'NN'), ('involves', 'NNS'), ('interaction', 'NN'), ('with', 'NN'), ('a', 'AT'), ('user', 'NN'), ('interface', 'NN'), ('or', 'NN'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Import all necessary Library and packag\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag import RegexpTagger\n",
    "#from nltk.corpus import brown\n",
    "\n",
    "#Perform word tokenize in order to enable each word tagging\n",
    "tk = word_tokenize(data2)  \n",
    "\n",
    "#Declare the Regular Expression for tagger\n",
    "# test_sent = brown.sents(categories='news')[0]\n",
    "reg_exp_tagger = RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # Cardinal Numbers\n",
    "     (r'(The|the|A|a|An|an)$', 'AT'),   # Articles\n",
    "     (r'.*able$', 'JJ'),                # Adjectives\n",
    "     (r'.*ness$', 'NN'),                # Nouns Formed from Adjectives\n",
    "     (r'.*ly$', 'RB'),                  # Adverbs\n",
    "     (r'.*s$', 'NNS'),                  # Plural Nouns\n",
    "     (r'.*ing$', 'VBG'),                # Gerunds\n",
    "     (r'.*ed$', 'VBD'),                 # Past Tense Verbs\n",
    "     (r'.*', 'NN')                      # Nouns (default)\n",
    "    ])\n",
    "\n",
    "#Tag the reg expression in token then Display the result\n",
    "print(reg_exp_tagger.tag(tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc81d6",
   "metadata": {},
   "source": [
    "## 4.3 Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93773de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Library\n",
    "import nltk\n",
    "\n",
    "#Download req packages\n",
    "#nltk.download('punkt')\n",
    "#ntlk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#Call req functions\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88f3f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('videogame', 'NN'), ('or', 'CC'), ('computergame', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('electronic-game', 'JJ'), ('that', 'WDT'), ('involves', 'VBZ'), ('interaction', 'NN'), ('with', 'IN'), ('a', 'DT'), ('user', 'JJ'), ('interface', 'NN'), ('or', 'CC'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the Sentence & POS Tagging the text\n",
    "tag = pos_tag(word_tokenize(data2))\n",
    "\n",
    "#Display Result\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb75fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CFG Form\n",
    "chk = RegexpParser(\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
    "P: {<IN>}            #To extract Prepositions\n",
    "V: {<V.*>}           #To extract Verbs\n",
    "PP: {<p> <NP>}       #To extract Prepositional Phrases\n",
    "VP: {<V> <NP|PP>*}   #To extract Verb Phrases\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bf8aec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Extracting \n",
      " (S\n",
      "  (NP A/DT videogame/NN)\n",
      "  or/CC\n",
      "  (NP computergame/NN)\n",
      "  (VP (V is/VBZ))\n",
      "  an/DT\n",
      "  electronic-game/JJ\n",
      "  that/WDT\n",
      "  (VP (V involves/VBZ) (NP interaction/NN))\n",
      "  (P with/IN)\n",
      "  (NP a/DT user/JJ interface/NN)\n",
      "  or/CC\n",
      "  (NP input/NN)\n",
      "  (NP device/NN))\n"
     ]
    }
   ],
   "source": [
    "#Print output in Lexical Form\n",
    "output_tag = chk.parse(tag)\n",
    "print(\"After Extracting \\n\", output_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad397faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw the CFG Diagram\n",
    "output_tag.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e75f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
